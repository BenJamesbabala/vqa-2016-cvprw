<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Visual Question Answering for CVPR 2016 by imatge-upc</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Visual Question Answering for CVPR 2016</h1>
      <h2 class="project-tagline">UPC team</h2>
      <a href="https://github.com/imatge-upc/vqa-2016-cvprw" class="btn">View on GitHub</a>
      <a href="https://github.com/imatge-upc/vqa-2016-cvprw/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/imatge-upc/vqa-2016-cvprw/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="visual-question-answering" class="anchor" href="#visual-question-answering" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visual Question Answering</h1>

<p>This is the project page of the UPC team participating in the <a href="http://www.visualqa.org/challenge.html">VQA challenge</a> for CVPR 2016. Details of the proposed solutions will be posted after the deadline.</p>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/issey_masuda.jpg" alt="Issey Masuda Mora" title="Issey Masuda Mora"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/xavier_giro.jpg" alt="Xavier Giró-i-Nieto" title="Xavier Giró-i-Nieto"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/santi_pascual.jpg" alt="Santiago Pascual de la Puente" title="Santiago Pascual de la Puente"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Main contributor</td>
<td align="center">Advisor</td>
<td align="center">Co-advisor</td>
</tr>
<tr>
<td align="center">Issey Masuda Mora</td>
<td align="center"><a href="https://imatge.upc.edu/web/people/xavier-giro">Xavier Giró-i-Nieto</a></td>
<td align="center">Santiago Pascual de la Puente</td>
</tr>
</tbody>
</table>

<p>Institution: <a href="http://www.upc.edu">Universitat Politècnica de Catalunya</a>.</p>

<p><img src="https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/upc_etsetb.jpg" alt="Universitat Politècnica de Catalunya"></p>

<h2>
<a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>

<p>Deep learning techniques have been proven to be a great success for some basic perceptual tasks like object detection and recognition. 
They have also shown good performance on tasks such as image captioning but these models are not that good when a higher reasoning is needed.</p>

<p>Visual Question-Answering tasks require the model to have a much deeper comprehension and understanding of the scene and the realtions between the objects
in it than that required for image captioning. The aim of these tasks is to be able to predict an answer given a question related to an image.</p>

<p>Different scenarios have been proposed to tackle this problem, from multiple-choice to open-ended questions. Here we have only addressed the
open-ended model.</p>

<h2>
<a id="what-are-you-going-to-find-here" class="anchor" href="#what-are-you-going-to-find-here" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What are you going to find here</h2>

<p>This project gives a baseline code to start developing on Visual Question-Answering tasks, specifically those focused on the <a href="http://www.visualqa.org/challenge.html">VQA challenge</a>. Here you will find
an example on how to implement your models with <a href="http://keras.io/">Keras</a> and train, validate and test them on the <a href="http://www.visualqa.org/download.html">VQA dataset</a>. Note that we are still building things upon this 
project so the code is not ready to be imported as a module but we would like to share it with the community to give a starting point for newcomers. </p>

<h2>
<a id="dependencies" class="anchor" href="#dependencies" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dependencies</h2>

<p>This project is build using the <a href="https://github.com/fchollet/keras">Keras</a> library for Deep Learning, which can use as a backend both <a href="https://github.com/Theano/Theano">Theano</a> 
and <a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>.</p>

<p>We have used Theano in order to develop the project and it has not been tested with TensorFlow.</p>

<p>For a further and more complete of all the dependencies used within this project, check out the requirements.txt provided within the project. This file will help you to recreate the exact
same Python environment that we worked with.</p>

<h2>
<a id="project-structure" class="anchor" href="#project-structure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project structure</h2>

<p>The main files/dir are the following:</p>

<ul>
<li>bin: all the scripts that uses the vqa module are here. This is your entry point.</li>
<li>data: the get_dataset.py script will download the whole dataset for you and place it where it can access it. Alternatively, you can provide the route of
the dataset if you have already downloaded it. The vqa module created some directory structure to place preprocessed files</li>
<li>vqa: this is a python package with the core of the project</li>
<li>requirements.txt: to be able to reproduce the python environment. You only need to do <code>pip install</code> in the project's root and it will install all
the dependencies needed</li>
</ul>

<h2>
<a id="the-model" class="anchor" href="#the-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The model</h2>

<p>We have participated into the <a href="http://www.visualqa.org/challenge.html">VQA challenge</a> with the following model. </p>

<p>Our model is composed of two branches, one leading with the question and the other one with the image, that are merged to predict the answer.
The question branch takes the question as tokens and obtains the word embedding of each token. Then, we feed these word embeddings into a LSTM and we take
its last state (once it has seen all the question) as our question representation, which is a sentence embedding.
For the image branch, we have first precomputed the visual features of the images with a Kernalized CNNs (KCNNs) [Liu 2015]. We project these features into
the same space as the question embedding using a fully-connected layer with ReLU activation function.</p>

<p>Once we have both the visual and text features, we merge them suming both vectors as they belong to the same space. This final representation is given to
another fully-connected layer softmax to predict the answer, which will be a one-hot representation of the word (we are predicting a single word as our answer).</p>

<p><img src="https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/model.jpg" alt="Model architecture"></p>

<h2>
<a id="related-work" class="anchor" href="#related-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related work</h2>

<ul>
<li>Ren, Mengye, Ryan Kiros, and Richard Zemel. <a href="http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering">"Exploring models and data for image question answering."</a> In Advances in Neural Information Processing Systems, pp. 2935-2943. 2015. <a href="http://gitxiv.com/posts/6pFP3b8gqxWZdBfjf/exploring-models-and-data-for-image-question-answering">[code]</a>
</li>
<li>Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html">"VQA: Visual question answering."</a> In Proceedings of the IEEE International Conference on Computer Vision, pp. 2425-2433. 2015. <a href="http://gitxiv.com/posts/zDn9kkA66FnG3ZuKz/vqa-visual-question-answering">[code]</a>
</li>
<li>Zhu, Yuke, Oliver Groth, Michael Bernstein, and Li Fei-Fei. <a href="http://web.stanford.edu/%7Eyukez/visual7w.html">"Visual7W: Grounded Question Answering in Images."</a> arXiv preprint arXiv:1511.03416 (2015).</li>
<li>Malinowski, Mateusz, Marcus Rohrbach, and Mario Fritz. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.html">"Ask your neurons: A neural-based approach to answering questions about images."</a> In Proceedings of the IEEE International Conference on Computer Vision, pp. 1-9. 2015. <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/">[code]</a>
</li>
<li>Xiong, Caiming, Stephen Merity, and Richard Socher. <a href="http://arxiv.org/abs/1603.01417">"Dynamic Memory Networks for Visual and Textual Question Answering."</a> arXiv preprint arXiv:1603.01417 (2016). <a href="https://news.ycombinator.com/item?id=11237125">[discussion]</a> <a href="http://www.nytimes.com/2016/03/07/technology/taking-baby-steps-toward-software-that-reasons-like-humans.html?_r=0">[Thew New York Times]</a>
</li>
<li>Serban, Iulian Vlad, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, and Yoshua Bengio. <a href="http://arxiv.org/abs/1603.06807">"Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus."</a> arXiv preprint arXiv:1603.06807 (2016). <a href="http://agarciaduran.org/">[dataset]</a>
</li>
</ul>

<h2>
<a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgements</h2>

<p>We would like to especially thank Albert Gil Moreno and Josep Pujal from our technical support team at the Image Processing Group at the UPC.</p>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/albert_gil.jpg" alt="Albert Gil" title="Albert Gil"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/josep_pujal.jpg" alt="Josep Pujal" title="Josep Pujal"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="web-albert">Albert Gil</a></td>
<td align="center"><a href="web-josep">Josep Pujal</a></td>
</tr>
</tbody>
</table>

<h2>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

<p>If you have any general doubt about our work or code which may be of interest for other researchers, please use the <a href="https://github.com/imatge-upc/vqa-2016/issues">issues section</a> 
on this github repo. Alternatively, drop us an e-mail at <a href="mailto:xavier.giro@upc.edu">xavier.giro@upc.edu</a>.</p>





      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/imatge-upc/vqa-2016-cvprw">Visual Question Answering for CVPR 2016</a> is maintained by <a href="https://github.com/imatge-upc">imatge-upc</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
